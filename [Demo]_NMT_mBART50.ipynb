{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tienhuynh96/Low_Resource_NMT/blob/main/%5BDemo%5D_NMT_mBART50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Low-resource Machine Translation using mBART50**"
      ],
      "metadata": {
        "id": "FvXEN84Yk8ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Import libraries**"
      ],
      "metadata": {
        "id": "BKA6YAxBjJUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPhRyDa7kjoM"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece datasets accelerate evaluate sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    MBart50TokenizerFast,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")"
      ],
      "metadata": {
        "id": "M0VZzEh-l3-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Dataset**"
      ],
      "metadata": {
        "id": "zZlBiTIkl8s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Incase can not load dataset from dataset**"
      ],
      "metadata": {
        "id": "8RWTm_QllgL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incase can not load dataset from dataset\n",
        "# Try\n",
        "# Wget data from github: https://github.com/stefan-it/nmt-en-vi\n",
        "# Train file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\"\n",
        "# Dev file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz\"\n",
        "# Test file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXbhkr00yYuk",
        "outputId": "c49ba4a0-0502-4d39-e2aa-b0e2e3bba0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-08 14:01:06--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/train-en-vi.tgz [following]\n",
            "--2024-06-08 14:01:06--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/train-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9903559 (9.4M) [application/octet-stream]\n",
            "Saving to: ‚Äòtrain-en-vi.tgz‚Äô\n",
            "\n",
            "train-en-vi.tgz     100%[===================>]   9.44M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-08 14:01:07 (162 MB/s) - ‚Äòtrain-en-vi.tgz‚Äô saved [9903559/9903559]\n",
            "\n",
            "--2024-06-08 14:01:07--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/dev-2012-en-vi.tgz [following]\n",
            "--2024-06-08 14:01:08--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/dev-2012-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103520 (101K) [application/octet-stream]\n",
            "Saving to: ‚Äòdev-2012-en-vi.tgz‚Äô\n",
            "\n",
            "dev-2012-en-vi.tgz  100%[===================>] 101.09K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-06-08 14:01:08 (36.4 MB/s) - ‚Äòdev-2012-en-vi.tgz‚Äô saved [103520/103520]\n",
            "\n",
            "--2024-06-08 14:01:08--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/test-2013-en-vi.tgz [following]\n",
            "--2024-06-08 14:01:08--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/test-2013-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102940 (101K) [application/octet-stream]\n",
            "Saving to: ‚Äòtest-2013-en-vi.tgz‚Äô\n",
            "\n",
            "test-2013-en-vi.tgz 100%[===================>] 100.53K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-06-08 14:01:09 (135 MB/s) - ‚Äòtest-2013-en-vi.tgz‚Äô saved [102940/102940]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the tgz file\n",
        "!tar -xzf train-en-vi.tgz\n",
        "!tar -xzf dev-2012-en-vi.tgz\n",
        "!tar -xzf test-2013-en-vi.tgz"
      ],
      "metadata": {
        "id": "CyNacfxolUyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add path for train, test, validation file\n",
        "# Train file\n",
        "train_source_file = \"/content/train.en\"\n",
        "train_target_file = \"/content/train.vi\"\n",
        "# Validation file\n",
        "val_source_file = \"/content/tst2012.en\"\n",
        "val_target_file = \"/content/tst2012.vi\"\n",
        "# Test file\n",
        "test_source_file = \"/content/tst2013.en\"\n",
        "test_target_file = \"/content/tst2013.vi\""
      ],
      "metadata": {
        "id": "DiS_Vv-ElUq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build function: Get examples from sourse, target file and append into data\n",
        "def generate_examples(source_file, target_file):\n",
        "    # Open and read source file and target file of \"train, test, validation\" data\n",
        "    with open(source_file, encoding=\"utf-8\") as f:\n",
        "        source_sentences = f.read().split(\"\\n\")\n",
        "    with open(target_file, encoding=\"utf-8\") as f:\n",
        "        target_sentences = f.read().split(\"\\n\")\n",
        "\n",
        "    # Add examples of \"train, test, validation\" data\n",
        "    data = []\n",
        "    source, target = \"en\", \"vi\"\n",
        "    for idx, (l1, l2) in enumerate(zip(source_sentences, target_sentences)):\n",
        "        result = {source: l1, target: l2}\n",
        "        data.append(result)\n",
        "    return data"
      ],
      "metadata": {
        "id": "OvqU14KxlUov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build data\n",
        "train_data = generate_examples(source_file=train_source_file, target_file=train_target_file)\n",
        "test_data = generate_examples(test_source_file, test_target_file)\n",
        "val_data = generate_examples(val_source_file, val_target_file)"
      ],
      "metadata": {
        "id": "-MkW-R3h0MLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# from datasets import DatasetDict to create data dict\n",
        "from datasets import DatasetDict\n",
        "# Import dataset to use code:datasets.Dataset\n",
        "# If from datasets import Dataset: error wil be raised, because it's dublicated with Dataset above\n",
        "import datasets\n",
        "\n",
        "# train data\n",
        "tem_dic = {}\n",
        "tem_dic['translation'] = train_data\n",
        "train_data = pd.DataFrame(tem_dic)\n",
        "train_data = datasets.Dataset.from_pandas(train_data)\n",
        "\n",
        "# # test data\n",
        "tem_dic = {}\n",
        "tem_dic['translation'] = test_data\n",
        "test_data = pd.DataFrame(tem_dic)\n",
        "test_data = datasets.Dataset.from_pandas(test_data)\n",
        "\n",
        "# # validation\n",
        "tem_dic = {}\n",
        "tem_dic['translation'] = val_data\n",
        "val_data = pd.DataFrame(tem_dic)\n",
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "9mnYEkgE42mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C95Rf1N02dzy",
        "outputId": "e3f9c577-b905-4adc-e4ec-a884a163e41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['translation'],\n",
              "    num_rows: 133318\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaet dataset_dict: have shape like load from dataset library\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_data,\n",
        "    'validation': val_data,\n",
        "    'test': test_data\n",
        "})"
      ],
      "metadata": {
        "id": "OPoa7-vz6sCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnFTqkd_63qp",
        "outputId": "3352a2c4-9ce6-47c1-eaca-d0c74d3f3fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 133318\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1554\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1269\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_dict['train']['translation'][0]\n",
        "dataset_dict['train']['translation'][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXrzVjS67K1n",
        "outputId": "1b33f1ff-f318-4e4b-ee5d-0b170850f1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Rachel Pike : The science behind a climate headline',\n",
              " 'vi': 'Khoa h·ªçc ƒë·∫±ng sau m·ªôt ti√™u ƒë·ªÅ v·ªÅ kh√≠ h·∫≠u'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. Create Poem Dataset**"
      ],
      "metadata": {
        "id": "p7arUgleuMM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NMT dataset\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, cfg, data, data_type=\"train\"):\n",
        "        super().__init__()\n",
        "        # Config\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Separate to source text and target texts\n",
        "        self.src_texts, self.tgt_texts = self.read_data(data, data_type)\n",
        "\n",
        "        # Convert source texts to ids\n",
        "        self.src_input_ids = self.texts_to_sequences(self.src_texts)\n",
        "        # Convert target texts to ids\n",
        "        self.labels = self.texts_to_sequences(self.tgt_texts)\n",
        "\n",
        "    # read data function to separate train data to source texts and target texts\n",
        "    def read_data(self, data, data_type):\n",
        "        # Get data \"train\"\n",
        "        data = data[data_type]\n",
        "        # Get list source texts from data['train']\n",
        "        src_texts = [sample[\"translation\"][self.cfg.src_lang] for sample in data]\n",
        "        # Get list target texts from data['train']\n",
        "        tgt_texts = [sample[\"translation\"][self.cfg.tgt_lang] for sample in data]\n",
        "        # Return list of src_texts, tgt_texts\n",
        "        return src_texts, tgt_texts\n",
        "\n",
        "    # texts_to_sequences to convert tokens to ids\n",
        "    def texts_to_sequences(self, texts):\n",
        "        # Output of tokenizer is input_ids and attention_mask\n",
        "        data_inputs = self.cfg.tokenizer(\n",
        "            texts,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.cfg.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # Return input_ids\n",
        "        return data_inputs.input_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.src_input_ids[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_input_ids)[0]"
      ],
      "metadata": {
        "id": "I_zua8Rrl7ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Config**"
      ],
      "metadata": {
        "id": "ydC6CtsJmVU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base configuration class meant to be inherited by other configuration classes\n",
        "class BaseConfig:\n",
        "    \"\"\" base Encoder Decoder config \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):     # Takes any number of keyword arguments and sets them as attributes of the instance using a loop\n",
        "        for k, v in kwargs.items():   # Iterates over the key-value pairs in the provided arguments.\n",
        "            setattr(self, k, v)       # Sets an attribute k with value v on the instance\n",
        "\n",
        "# NMTConfig class: defines a specific configuration for a Neural Machine Translation (NMT) task, inheriting from BaseConfig.\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'en'\n",
        "    tgt_lang = 'vi'\n",
        "    max_len = 75\n",
        "    add_special_tokens = True\n",
        "\n",
        "    # Model\n",
        "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "    # Training\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    learning_rate = 5e-5\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16\n",
        "    num_train_epochs = 2\n",
        "    save_total_limit = 1    # Limit on the total number of checkpoints to save\n",
        "    ckpt_dir = f'./mbart50-{src_lang}-{tgt_lang}' # Directory path for saving model checkpoints\n",
        "    eval_steps = 1000       # Number of steps between evaluations\n",
        "\n",
        "    # Inference\n",
        "    beam_size = 5           # Beam size for beam search during inference\n",
        "\n",
        "# initializing cfg with the default values specified in the class definition.\n",
        "cfg = NMTConfig()"
      ],
      "metadata": {
        "id": "IkqlqGRXmT8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenizer, Model, Metric**"
      ],
      "metadata": {
        "id": "skgyEEm2N8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name, src_lang=\"en_XX\",tgt_lang = \"vi_VN\")\n",
        "# Creating a new attribute tokenizer in the cfg object\n",
        "cfg.tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name)\n",
        "# Create model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)"
      ],
      "metadata": {
        "id": "vPTFG2dFT8ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uFiRdWiO4Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sacrebleu for metric\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Create function to process labels and prediction (use strip)\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "# Create compute metric function\n",
        "def compute_metrics(eval_preds):\n",
        "    # Get prediction and labels from evaluation prediciton results (a tuple containing predictions and labels)\n",
        "    preds, labels = eval_preds    # preds: N x S, labels: N x S\n",
        "    # If preds is a tuple, it takes the first element. This is common in some models that return additional outputs such as attention scores.\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Replace the token -100 in preds with the tokenizer's pad token ID (the -100 token is used to mark ignored positions)\n",
        "    preds= np.where(preds != -100, preds, cfg.tokenizer.pad_token_id)\n",
        "    # Decode the token ids to text and skips special tokens and cleans up tokenization spaces.\n",
        "    decoded_preds = cfg.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Replace the token -100 in preds with the tokenizer's pad token ID (the -100 token is used to mark ignored positions)\n",
        "    labels= np.where(labels != -100, labels, cfg.tokenizer.pad_token_id)\n",
        "    # Decode labels\n",
        "    decoded_labels = cfg.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Calls the postprocess_text function to strip whitespace and format the labels appropriately.\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    # Compute BLEU Score\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    # Calculate Average Prediction Length (ignore padding)\n",
        "    prediction_lens = [np.count_nonzero(pred != cfg.tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    # Round Metric Values\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Df8-9v2Sm1to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training**"
      ],
      "metadata": {
        "id": "FSlhbfxDm69u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the datasets\n",
        "train_dataset = NMTDataset(cfg, dataset_dict, data_type=\"train\")\n",
        "valid_dataset = NMTDataset(cfg, dataset_dict, data_type=\"validation\")\n",
        "test_dataset = NMTDataset(cfg, dataset_dict, data_type=\"test\")\n"
      ],
      "metadata": {
        "id": "PYc1VlWGm8tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataset))"
      ],
      "metadata": {
        "id": "O3P0AynmpGnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e165f74-82a5-4287-b706-bbb5f42b9c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([250004, 127055,  66937,     13,    152,    581,  41664,  50155,     10,\n",
              "         153552,  10336,   2256,      2,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1]),\n",
              " 'labels': tensor([250004,  67766,   2546, 218877,    858,    889,  10037,   6248,   1893,\n",
              "          17964,  42254,      2,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "              1,      1,      1])}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training argument\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy='steps',\n",
        "    save_steps=cfg.eval_steps,\n",
        "    eval_steps=cfg.eval_steps,\n",
        "    output_dir=cfg.ckpt_dir,\n",
        "    per_device_train_batch_size=cfg.train_batch_size,\n",
        "    per_device_eval_batch_size=cfg.eval_batch_size,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    save_total_limit=cfg.save_total_limit,\n",
        "    num_train_epochs=cfg.num_train_epochs,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# initialize a data collator to handle the preparation of batches of data for sequence-to-sequence models\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    cfg.tokenizer,\n",
        "    model=model\n",
        ")\n",
        "\n",
        "# Set trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=cfg.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "QEo4pL4DnBbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600b964b-8452-4cb4-a98a-25921f0b39c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "G7PUkJdrpXgb",
        "outputId": "675a2f5e-4230-4fc6-ddf4-8f4d863e1119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3553' max='16666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3553/16666 2:04:33 < 7:39:58, 0.48 it/s, Epoch 0.43/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.509100</td>\n",
              "      <td>0.548572</td>\n",
              "      <td>28.487600</td>\n",
              "      <td>28.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.501100</td>\n",
              "      <td>0.538401</td>\n",
              "      <td>28.560300</td>\n",
              "      <td>28.012900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.492200</td>\n",
              "      <td>0.528043</td>\n",
              "      <td>29.159700</td>\n",
              "      <td>27.596500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check prediction\n",
        "prediction = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2WBHM4KuxLwK",
        "outputId": "d25c4547-3250-40ef-abcc-36e5b55908e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check prediction ans score\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvCK7O_O5ypJ",
        "outputId": "08f49e7e-62f3-4ed7-e7ad-9fc9969e6057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[     2, 250004,  16584, ...,      1,      1,      1],\n",
              "       [     2, 250004,  23598, ...,      1,      1,      1],\n",
              "       [     2, 250004,  71717, ...,      1,      1,      1],\n",
              "       ...,\n",
              "       [     2, 250004,  14343, ...,      1,      1,      1],\n",
              "       [     2, 250004, 131785, ...,      1,      1,      1],\n",
              "       [     2, 250004,      2, ...,      1,      1,      1]]), label_ids=array([[250004,  16584,   2259, ...,      1,      1,      1],\n",
              "       [250004,  14343,   1408, ...,      1,      1,      1],\n",
              "       [250004,  71717,   4373, ...,      1,      1,      1],\n",
              "       ...,\n",
              "       [250004,  14343,   1274, ...,      1,      1,      1],\n",
              "       [250004, 131785,  43209, ...,      1,      1,      1],\n",
              "       [250004,      2,      1, ...,      1,      1,      1]]), metrics={'test_loss': 0.5306801795959473, 'test_bleu': 34.8714, 'test_gen_len': 32.792, 'test_runtime': 134.5483, 'test_samples_per_second': 9.432, 'test_steps_per_second': 0.595})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inference**"
      ],
      "metadata": {
        "id": "i-6WGk_WXERC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "def inference(\n",
        "    text,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device=\"cpu\",\n",
        "    max_length=75,\n",
        "    beam_size=5\n",
        "    ):\n",
        "    # Tokenize input => input_ids and attention_mask\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    # Move input_ids to device\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    # Move attention_mask to device\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate outputs\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True,\n",
        "        num_beams=beam_size,\n",
        "        length_penalty=2.0\n",
        "    )\n",
        "\n",
        "    # Decode outputs\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return output_str"
      ],
      "metadata": {
        "id": "qL0L2ikojKXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i go to school'\n",
        "inference(sentence, cfg.tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN1-AweFkUlL",
        "outputId": "c979d095-d567-47ad-beb5-7b10c3fcdcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t√¥i ƒëi h·ªçc.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Checkpoint**\n",
        "https://drive.google.com/drive/folders/1ii_lPm2-1CfIhQM8RVzLgTHMxXDKgnk4?usp=sharing"
      ],
      "metadata": {
        "id": "zUEQ3I66cmBa"
      }
    }
  ]
}